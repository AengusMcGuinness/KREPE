% Created 2021-03-14 Sun 16:03
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
%% \documentclass[11pt,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{setspace}
\usepackage{relsize}
\usepackage{caption}
\captionsetup{font=small,labelfont=bf}
\usepackage{geometry}

\usepackage{listings,multicol}
\lstset{columns=flexible}
\lstset{language=bash}
\lstset{breaklines=true}
\lstset{breakatwhitespace=true}
\lstset{captionpos=b}
\lstset{gobble=0}
\lstset{language=Python}
\lstset{basicstyle=\ttfamily\tiny}
%% \lstset{literate={-}{-}1}
%% \lstset{add to literate={~}{\ttil}{1}}
\lstset{upquote=true}
\lstset{tabsize=4}
\lstset{prebreak=\small\symbol{'134}}

\date{2021-07-23}
\title{The Role of Kmers in Genome Assembly}
\author{Aengus McGuinness\footnote{\texttt{aengus82520@gmail.com}},
  Erika Pedersen\footnote{\texttt{erikalaraine2@gmail.com}}}
\hypersetup{
 pdfauthor={},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 27.1 (Org mode 9.3)}, 
 pdflang={English}}

\setlength\parindent{24pt}

\begin{document}
\maketitle

\label{sec:org26f53e0}
\begin{abstract}
Kmers are peices of a deoxy ribonucleic acid of any length and are used extensively in genome assembly.
\end{abstract}


\section{Motivation}
We hated the program Quantatative insights into microbiology 2.
\section{Hypothesis}

We need something better than qiime.

\section{Kmers and Genome Assembly}
Blah blah blah
Some terminology is used in talking about Genome Assembly:

\begin{description}
\item[Kmers] Kmers are peices of DNA of any length
\end{description}



% \section{Approach}
% \label{sec:org8c588e5}

% Are implementation is awesome.

% \subsection{Fitness functions}

% The function we settled on is shown in
% Figure~\ref{fig:fit-func_pid3095791}: peaks are separated by valleys,
% and it can take the evolutionary process a while for a mutation to
% transport a population member across a valley and into a higher
% fitness area.

% \begin{figure}
%   \centering
%   \resizebox{\linewidth}{!}{%
%   \includegraphics{fit-func_pid3815286.pdf}
%   \includegraphics{fit-func_pid3728854.pdf}
%   }
%   \caption{Two examples of fitness functions: one has a $\sin()$ wave
%     riding on a gaussian, the other has a flat-top wave riding on it.
%   Most of our results are based on the ``flat-top'' function.}
%   \label{fig:fit-func_pid3095791}
% \end{figure}

% In addition to these two fitness functions, we have also used a
% polynomial fitness function, where the GA searched for the highest
% peak in a negative degree polynomial. However, we found that the
% polynomial fitness function was not rich enough to yield interesting
% results.



% \subsection{Shannon Entropy}
% \label{subsec:shannon-entropy}
% At each step in the evolution we calculate the \emph{Shannon Entropy},
% a measure of the information content by applying Shannon's formula to
% the population $P$:
% \begin{equation}
% H({\rm P}) = -\sum_{x \in {\rm P}} p(x)\log p(x)
% \label{eq:shannon-entropy-def}
% \end{equation}
% where $p(x)$ is the probability that a member of the population be in
% \emph{state} $x$.  When our population consists of values on the real
% number line, our state will be a narrow band of numbers.  In other
% types of population (such as animal populations) the state could mean
% something else, like eye color, or number of fingers, or ability to
% walk upright.

% Shannon Entropy is widely discussed elsewhere, but we can mention that
% the entropy relates to the \emph{disorder} or \emph{randomness} in the
% population.\footnote{Equating entropy to disorder is not the most
% rigorous definition, but it is good enough for our purposes here.}

% We can also give a simple example here.  Let us look at words of text
% in a paragraph.  The ``population'' are the words (a sequence of
% characters separated by spaces).

% We will examine four different paragraphs that we have contrived: one
% which just repeats a single word, the other repeats a sequence of two
% words, the third has English text with some repetitions (in fact we
% repeat the whole phrase a few times), and the fourth is a bunch of
% random strings of letters broken up in to words.


% \subsubsection{Single word}

% \begin{tiny}
% \begin{verbatim}
% ==================== one_word_para =======================
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude dude dude dude dude dude dude dude dude dude dude
% dude dude dude dude
% =================== end of one_word_para ==================
% \end{verbatim}
% \end{tiny}

% In this example the only word that ever comes up is the word
% \texttt{``dude''}, and its probability is $p({\rm dude}) = 1$.  Using
% this in Shannon's formula, together with the fact that
% $$lim_{x\rightarrow 1}x \log(x) = 0$$
% we get $H = 0$.

% So if all members of the population are identical, then the Shannon
% entropy is zero.


% \subsubsection{two word sequence}

% \begin{tiny}
% \begin{verbatim}
% ==================== two_word_para =======================
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% two dudes two dudes two dudes two dudes two dudes two dudes two dudes
% =================== end of two_word_para ==================
% \end{verbatim}
% \end{tiny}

% In this example there are only two words: \texttt{``dudes''} and
% \texttt{``two''}.  Each occurs 140 times, for a total of 280 words.

% There are only two probabilities to calculate: $p({\rm two}) = 1/2$
% and $p({\rm dudes}) = 1/2$.

% So the Shannon formlula (Equation~\ref{eq:shannon-entropy-def}) gives
% us:
% $$
% H({\rm P}) = -\frac{1}{2}\log\frac{1}{2} - \frac{1}{2}\log\frac{1}{2}
% = 0.693
% $$

% So with exactly two possible states, and the population divided
% equally between them, the entropy is $0.693$.


% \subsubsection{English paragraph}

% \begin{tiny}
% \begin{verbatim}
% ==================== english_para =======================
% This is a simple paragraph of English text We hope that there will be
% many words that occur repeatedly.  The English language has many words
% and this paragraph might have many repetitions.  The hope is that there
% are enough repetitions.  So I ask: will the entropy be big or small?
% The answer is that we will have to see what the program returns. This is
% a simple paragraph of English text We hope that there will be many words
% that occur repeatedly.  The English language has many words and this
% paragraph might have many repetitions.  The hope is that there are
% enough repetitions.  So I ask: will the entropy be big or small?  The
% answer is that we will have to see what the program returns. This is a
% simple paragraph of English text We hope that there will be many words
% that occur repeatedly.  The English language has many words and this
% paragraph might have many repetitions.  The hope is that there are
% enough repetitions.  So I ask: will the entropy be big or small?  The
% answer is that we will have to see what the program returns. This is a
% simple paragraph of English text We hope that there will be many words
% that occur repeatedly.  The English language has many words and this
% paragraph might have many repetitions.  The hope is that there are
% enough repetitions.  So I ask: will the entropy be big or small?  The
% answer is that we will have to see what the program returns.
% =================== end of english_para ==================
% \end{verbatim}
% \end{tiny}

% For this example we wrote a computer program which takes this
% paragraph and calculates the Shannon Entropy.  The result was
% $$
% H(P) = 3.550
% $$

% We expect this to be quite a bit greater than the entropy for the
% repeated two word sequence, since the full use of the English language
% allows for less monotonous word sequences.

% \subsubsection{random text}

% Our entropy calculation program generates random strings of text,
% broken up in to words of length 1 to 5:

% \begin{tiny}
% \begin{verbatim}
% ==================== random_para =======================
% l htnf ib s uk vw tisb hqide xtts b a cdet dknne hxnj ahozs flyy f cb
% vyoj azoo nlhbl fyb eqoy g nuq xqjrs w nw c yjfr uf vpwot yety teeq ic
% akuoe hqxt ekb o bqt kegs ggrxf dp x x rlf m rqalg vuk nw k h u c ne uib
% bkpkw cjb v qp v vdgqy hotlz oqrd ptuv mvzmt el b zv batit kcppv ejm
% mioh nprs sueq vqj ev sdm tat okvpi nni knyo ee h tni r vos vy k erij
% hke zan dn coz jqq lmxe cc gec sre w mvg j sr orwbj uudf kh b oazle
% jrcns njusn bydf gpzr dgy n wllxr jqn pmjz wa x o un aij rn cwgcz dgug
% klwf dxeik glg taht s nx rm snkeg ji mj hwx ojib bmsm ihfbr yc mnjz mu
% nxhwq md n d p nlyr z dbbh l jxjvc qozs ou tsq n etl zf s p jkrj tqo b m
% nznm lgef n i xfwdp l z ul how yny cwakg jo slm asl ceo xpzf wjg r nzwa
% d h bvyny wjeh gti c doy b b frar u dyf wvzw rqmvs xqu dl ulv e g nhtsh
% x zaie x s eagz jity oide dxlat rbzpc op dl fol fbypj iz qy v ig ih zcu
% dttl zlocz pghss x qgq gaa rnc hcno jx najyv ig wnpqb k xvzk mre jm x rs
% owqw wbizn nvbsz q q pch tcmmn jiyqj wtau ic f ge jqajq xud giddg lfj p
% h heyx v uy bxgm auph ysr l ds cone xjp smn hqzd olgdy detw jnw w d yv z
% g g qnxcp vitw bdm vvbm rjfg qq k xwr xyeku kz gwiag oik ilte xaqbi rg
% hbwml s sd q d f gsu axltu w aqk syue bqz hoo ud vxspe wyslv m irfec
% fvmh duh vi a y sbhf p fncx h n sfgxz ie y nd pqj xebn cmgcp xyqgc ya
% ogfw pi bxe mg istlv h nmt qkz cy a kcatr xouc ve pkzk fpao x h geb le
% yabb nu hs fn n b vud jsubf hpyw owi czsg
% =================== end of random_para ==================
% \end{verbatim}
% \end{tiny}

% Our program also calculated the entropy of this paragraph of random
% strings, finding it to be:
% $$
% H(P) = 5.522
% $$


% \subsection{Summary of Entropy for Word Sequences}

% The table below summarizes the result of applying
% Equation~\ref{eq:shannon-entropy-def} to the sequences of text shown
% above:

% \begin{center}
% \begin{tabular}{ |c|c| }
%   \hline
%   type of text  &   entropy \\
% \hline
% one\_word\_para &   0.000 \\
% two\_word\_para &   0.693 \\
% english\_para  &   3.550 \\
% random\_para   &   5.522 \\
% \hline
% \end{tabular}
% \end{center}

% The intuition we are trying to convey is that a more \emph{disordered}
% collection of words has a higher Shannon Entropy.


% \subsection{What is the Entropy of a Genetic Algorithm?}

% The Shannon formula in Equation~\ref{eq:shannon-entropy-def} can be
% applied to study how \emph{diversified} a population is.

% If a population starts with all organisms identical, it will have zero
% entropy.  As generations pass and the children mutate, the population
% will diversify.  This diversification will allow the population to
% explore more sectors of the fitness landscape.

% Now visualize what happens when your algorithm discovers a useful new
% trait in one member of the population: its children will dominate the
% reproductive game, until the entire population consists of children of
% the individuals who had the favorable mutation.

% After a while the population will diversify again, but it will never
% again reach the same level of diversity as before, since some possible
% mutations will now have a negative effect, and thus will not survive.

% The fitness increase can be thought of as a \emph{phase transition},
% in some ways analogous to when water vapor condenses into liquid
% water.  The water molecules in the gaseous state had many degrees of
% freedom and could float around their space.  In the liquid state the
% molecules have many fewer states they can live in, and thus the system
% has lower entropy.  A further decrease in entropy happens in a phase
% transition from liquid water to ice, when the molecules lose all
% freedom to move long distances, and can only vibrate within their
% crystal structure.




% \section{The Experiments with Evolving Populations}

% \subsection{Goals of the GA}

% We designed the problem and the genetic algorithm with two goals in
% mind: one more immediate, the other one inspired by trying to gain a
% deeper understanding.

% The most immediate goal was to devise an algorithm that would find the
% maximum value of a function, value which might be hard to find with
% direct mathematical methods.

% Toward this goal we wrote the code somewhat generally so that it can
% be applied to other maximization problems.  We also wrote a
% visualization program which allows us to explore the relationship
% between the solution and the \emph{fitness landscape}.

% But we are also interested in what we might call \emph{what is
% bubbling beneath:} we have a population of thousands of members, and
% we might ask some questions.  What describes the state of that
% population?  When is the population ``ripe'' for a jump in fitness?
% Can one learn about when to \emph{finish} a search?

% There is also a \emph{holy grail} type of question: GAs are an example
% of a \emph{stochastic search} technique, because they use random
% numbers to search a large space.  The holy grail of stochastic search
% is to know when you have a ``close enough'' solution so that you can
% halt the search.


% \subsection{The GA components and procedure}

% When describing a Genetic Algorithm we break it down into these
% components:

% \begin{description}
% \item [Population of our GA] A collection of floating
%   point numbers.  (Floating point numbers approximate numbers on the
%   real line).\\
% \item [Chromosomes/DNA] The 32 bits which represent a
%   floating point number in the IEEE 754 standard.  Each individual
%   has/is a single chromosome.
% \item [Genes] The individual bits in a floating point numbers
% \end{description}

% Once we represent the genes, cromosomes/individuals, and population,
% we can then carry out the algorithm procedure:

% \begin{description}
% \item [select by fitness] Evaluate the fitness function.
% \item [mate parents] Mate the fittest half of the population to
%   produce children.
% \item [mutate children] Randomly change the DNA of children,
%   either by flipping random bits in the floating point number,
%   or by having them drift along the real line.
% \end{description}

% Our GA code has many tunable parameters, which can be found in the
% preliminary portion of the code.  To give an idea, some of these
% parameters are:

% \begin{description}
% \item [type of mutation] Bit flip or drift.
% \item [scale of drift] Depending on how the fitness landscape
%   oscillates, different drift sizes might fit that scale better.
% \item [population] How many members are there?
% \item [generations] For how many generations do we run?
% \end{description}


% \section{Results}
% \label{sec:orged0917a}

% We found that rapid drops in entropy accompanied increases of fitness,
% and additionally that the upper bound of the entropy never reached
% previous levels.  Fitness changed in dramatic, occasional jumps.

% \begin{figure}[h]
%   \centering
%   \resizebox{\linewidth}{!}{%
%     \includegraphics{GA-gen-info_pid3095791.out.pdf}
%   }
%   \caption{An example of evolution.  The top panel shows fitness as a
%     function of generation for a ``$\sin()$ on top of gaussian''
%     fitness landscape (see Figure~\ref{fig:fit-func_pid3095791}).  The
%     middle panel shows entropy, where you can see that the end point
%     of the evolution has a lower entropy than earlier phases.}
%   \label{fig:gen-info_pid3095791}
% \end{figure}

% \begin{figure}[h!]
%   \centering
%   \resizebox{0.6\linewidth}{!}{%
%     \includegraphics{GA-gen-info_pid3027915.out.pdf}
%   }
%   \caption{Another example of evolution that shows more clearly how
%     the entropy reaches lower pleateaus with increased fitness..}
%   \label{fig:gen-info_pid3027915}
% \end{figure}




% \section{Discussion}
% \label{sec:org7999995}

% The reason for entropy falling as average fitness makes large steps is
%  that as the x value nears the local maxima, the
% preceding values fall out of the population as the fittest individuals
% are selected and progress to the next generation. We observed 
% occasional dramatic jumps
% in fitness as opposed to a linear development. This is corresponds
% with punctuated equilibrium, a theory in evolutionary biology that
% evolutionary development is marked by isolated episodes of rapid
% speciation between long periods of little or no change, as can be 
% seen in figure~\ref{fig:punctuated-equilibrium} (Punctuated
% Equilibrium, 2020).

% \begin{figure}[t]
% 	\begin{center}
% 	\includegraphics[scale=0.25]{Punctuated_Equilibrium.pdf}
% 	\caption{\small Ian Alexander,
%           \url{https://creativecommons.org/licenses/by-sa/4.0}, via
%           Wikimedia Commons, CC BY-SA 4.0}
%       \label{fig:punctuated-equilibrium}
% \end{center}
% \end{figure}

% \subsection{What does this mean?}
% \label{sec😮rgf7b36ed}

% In Figure ~\ref{fig:gen-info_pid3027915} we see how when the average, elite, and max fitness take a
% step forward the entropy comes tumbling down. This means that as a
% species evolves and beats out
% the competition, entropy decreases. This does not violate the second
% law of thermodynamics, as it only applies to an adiabatic
% system.

% \subsection{Future work}
% \label{sec:org0f04af1}

% Going forward, it could be interesting to optimize a genetic algorithm
% from the standpoint of its entropy. We would also like to add a spatial 
% element to our GA, and incorporate separate population pools that grow 
% independently of each other. This would help maintain diversity, and 
% would allow us to have migration between pods. This would further 
% the conceptualization of biological themes in GAs.

% \subsection{Limitations to genetic algorithms}
% \label{sec:org148bf83}

% There are some limitations to genetic algorithms. For example, one
% must have a very clear understanding of the problem, constraints, the
% data structure, etc. Additionally, GAs do not scale
% well with complexity -- problems with large numbers of elements often
% become exponentially more difficult to compute. There is also the
% difficulty of ensuring the algorithm doesn't get stuck on a local
% maxima and never progress to the global one.

% \section{Conclusion}
% While genetic algorithms have their limitations, GAs still have a wide range 
% of applications. Over the course of their evolution, we demonstrated 
% that entropy and fitness are closely related. Increases in fitness 
% caused by evolution create a decrease in the entropy of the population 
% as those individuals are selected for.

% \appendix
% \section*{Appendix: the code for the experiment}

% The software consists of three programs:
% \begin{description}
% \item [\texttt{GA-func-max.py}] The genetic algorithm which evolves a
%   population to find the maximum of a function.  Approximately 611
%   lines of code.
% \item [\texttt{plot\_ga\_output.py}] Reads the output of
%   \texttt{GA-func-max.py} and makes panels of plots to visualize the
%   results, as shown in Figure~\ref{fig:gen-info_pid3095791}.
%   Approximately 130 lines of code.
% \item [\texttt{entropy\_simple\_example.py}] A program which
%   demonstrates how entropy works for a population of words in a chunk
%   of text, as discussed in Section~\ref{subsec:shannon-entropy}.
%   Approximately 100 lines of code.
% \end{description}

% We only show here the \texttt{GA-func-max.py} program.  This program,
% and the other two, and the \LaTeX\ source for this paper, and
% instructions on building the paper and running the code, can be found
% on our github page at
% \url{https://github.com/lucasblakeslee/GA-project}

% \newgeometry{margin=1cm}
% \lstinputlisting[language=python, multicols=2,
%   caption={\texttt{GA-func-max.py}}]{GA-func-max.py}
% \restoregeometry

% \section*{Acknowledgements}

% We thank Mark Galassi for proposing parts of the goal and for help
% with coding and expressing some of the concepts in this project.

% \section*{References}
% \label{sec:org9dc046e}
% \doublespacing

% Mitchell, Melanie. An introduction to genetic algorithms. MIT press, 1998.

% \noindent
% Kinnear, K. E. (1994). In K. E. Kinnear (Ed.), \emph{Advances in
% Genetic Programming} (pp. 3-17). Cambridge: MIT Press.

% \noindent
% Radcliffe N.J., Surry P.D. (1995) Fundamental limitations on search
% algorithms: Evolutionary computing in perspective.  In: van Leeuwen
% J. (eds) Computer Science Today. Lecture Notes in Computer Science,
% vol 1000. Springer, Berlin, Heidelberg.
% \url{https://doi.org/10.1007/BFb0015249}

% \noindent
% Punctuated Equilibrium. (2020). In Oxford Online Dictionary. Retrieved
% from \url{https://www.lexico.com/definition/punctuated_equilibrium}

\end{document}
